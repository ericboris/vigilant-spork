Method: what kind of method will you use, and how will you implement it (e.g. language, framework)?As a placeholder we're currently using an implementation of A2's MLE trained on nltk.corpus words dataset. However, in future iterations, we plan to use a Transformer and implement it by using the XMLM-RoBERTa. This includes two options for casual language modeling: next token prediction from a sequence as well as token generation from a prior sequence. This will need modification to predict a specific character rather than an entire token. We will likely use PyTorch.Dataset: what kind of data are you going to use to train your model, and how will you obtain this data?The benefit of a transformer from our research is that it can be trained with unsupervised data. The XMLM-RoBERa is trained on 2.5TB of data in 100 languages, including low-resource languages. There is no need to obtain the data since the model is pretrained. References:https://www.youtube.com/watch?v=S27pHKBEp30https://huggingface.co/transformers/task_summary.htmlhttps://huggingface.co/transformers/model_doc/roberta.html#robertaforcausallm
